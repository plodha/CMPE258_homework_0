{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_classification_with_hub.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"2ew7HTbPpCJH","colab":{}},"source":["import numpy as np\n","\n","import tensorflow as tf\n","\n","!pip install tensorflow-hub\n","!pip install tfds-nightly\n","import tensorflow_hub as hub\n","import tensorflow_datasets as tfds\n","\n","print(\"Version: \", tf.__version__)\n","print(\"Eager mode: \", tf.executing_eagerly())\n","print(\"Hub version: \", hub.__version__)\n","print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iAsKG535pHep"},"source":["## Download the IMDB dataset\n","\n","The IMDB dataset is available on [imdb reviews](https://www.tensorflow.org/datasets/catalog/imdb_reviews) or on [TensorFlow datasets](https://www.tensorflow.org/datasets). The following code downloads the IMDB dataset to your machine (or the colab runtime):"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zXXx5Oc3pOmN","colab":{}},"source":["# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\n","# for training, 10,000 examples for validation and 25,000 examples for testing.\n","train_data, validation_data, test_data = tfds.load(\n","    name=\"imdb_reviews\", \n","    split=('train[:60%]', 'train[60%:]', 'test'),\n","    as_supervised=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"l50X3GfjpU4r"},"source":["## Explore the data \n","\n","Let's take a moment to understand the format of the data. Each example is a sentence representing the movie review and a corresponding label. The sentence is not preprocessed in any way. The label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review.\n","\n","Let's print first 10 examples."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QtTS4kpEpjbi","colab":{}},"source":["train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n","train_examples_batch"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IFtaCHTdc-GY"},"source":["Let's also print the first 10 labels."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tvAjVXOWc6Mj","colab":{}},"source":["train_labels_batch"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LLC02j2g-llC"},"source":["## Build the model\n","\n","The neural network is created by stacking layers—this requires three main architectural decisions:\n","\n","* How to represent the text?\n","* How many layers to use in the model?\n","* How many *hidden units* to use for each layer?\n","\n","In this example, the input data consists of sentences. The labels to predict are either 0 or 1.\n","\n","One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have three advantages:\n","\n","*   we don't have to worry about text preprocessing,\n","*   we can benefit from transfer learning,\n","*   the embedding has a fixed size, so it's simpler to process.\n","\n","For this example we will use a **pre-trained text embedding model** from [TensorFlow Hub](https://www.tensorflow.org/hub) called [google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1).\n","\n","There are three other pre-trained models to test for the sake of this tutorial:\n","\n","* [google/tf2-preview/gnews-swivel-20dim-with-oov/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1) - same as [google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1), but with 2.5% vocabulary converted to OOV buckets. This can help if vocabulary of the task and vocabulary of the model don't fully overlap.\n","* [google/tf2-preview/nnlm-en-dim50/1](https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1) - A much larger model with ~1M vocabulary size and 50 dimensions.\n","* [google/tf2-preview/nnlm-en-dim128/1](https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1) - Even larger model with ~1M vocabulary size and 128 dimensions."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"In2nDpTLkgKa"},"source":["Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples. Note that no matter the length of the input text, the output shape of the embeddings is: `(num_examples, embedding_dimension)`."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_NUbzVeYkgcO","colab":{}},"source":["embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n","hub_layer = hub.KerasLayer(embedding, input_shape=[], \n","                           dtype=tf.string, trainable=True)\n","hub_layer(train_examples_batch[:3])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dfSbV6igl1EH"},"source":["Let's now build the full model:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xpKOoWgu-llD","colab":{}},"source":["model = tf.keras.Sequential()\n","model.add(hub_layer)\n","model.add(tf.keras.layers.Dense(16, activation='relu'))\n","model.add(tf.keras.layers.Dense(1))\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6PbKQ6mucuKL"},"source":["The layers are stacked sequentially to build the classifier:\n","\n","1. The first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The pre-trained text embedding model that we are using ([google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1)) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: `(num_examples, embedding_dimension)`.\n","2. This fixed-length output vector is piped through a fully-connected (`Dense`) layer with 16 hidden units.\n","3. The last layer is densely connected with a single output node.\n","\n","Let's compile the model."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"L4EqVWg4-llM"},"source":["### Loss function and optimizer\n","\n","A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs logits (a single-unit layer with a linear activation), we'll use the `binary_crossentropy` loss function.\n","\n","This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`. But, generally, `binary_crossentropy` is better for dealing with probabilities—it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n","\n","Later, when we are exploring regression problems (say, to predict the price of a house), we will see how to use another loss function called mean squared error.\n","\n","Now, configure the model to use an optimizer and a loss function:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Mr0GP-cQ-llN","colab":{}},"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"35jv_fzP-llU"},"source":["## Train the model\n","\n","Train the model for 20 epochs in mini-batches of 512 samples. This is 20 iterations over all samples in the `x_train` and `y_train` tensors. While training, monitor the model's loss and accuracy on the 10,000 samples from the validation set:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tXSGrjWZ-llW","colab":{}},"source":["history = model.fit(train_data.shuffle(10000).batch(512),\n","                    epochs=20,\n","                    validation_data=validation_data.batch(512),\n","                    verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9EEGuDVuzb5r"},"source":["## Evaluate the model\n","\n","And let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zOMKywn4zReN","colab":{}},"source":["results = model.evaluate(test_data.batch(512), verbose=2)\n","\n","for name, value in zip(model.metrics_names, results):\n","  print(\"%s: %.3f\" % (name, value))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z1iEXVTR0Z2t"},"source":["This fairly naive approach achieves an accuracy of about 87%. With more advanced approaches, the model should get closer to 95%."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5KggXVeL-llZ"},"source":["## Further reading\n","\n","For a more general way to work with string inputs and for a more detailed analysis of the progress of accuracy and loss during training, take a look [here](https://www.tensorflow.org/tutorials/keras/basic_text_classification)."]}]}