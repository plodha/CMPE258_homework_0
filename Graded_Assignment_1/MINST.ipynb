{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MINST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rer0ZiQr2hhN",
        "colab_type": "text"
      },
      "source": [
        "# CMPE 258 Assignment 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eOQXwL1hrQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc5H_f202EL1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0c13a824-5547-488a-d707-b077dae7f766"
      },
      "source": [
        "# Connect Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pAjMW5B20Hb",
        "colab_type": "text"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD_ATbIm219N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa9a435c-ca0f-46c6-db3c-125fc5e92b2d"
      },
      "source": [
        "train =  np.genfromtxt('train.csv', delimiter=\",\", skip_header=1)\n",
        "train.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(59999, 785)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lufzo4l44e3I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dadedf06-a82b-4b48-be41-be45e974f48b"
      },
      "source": [
        "test =  np.genfromtxt('test.csv', delimiter=\",\", skip_header=1)\n",
        "test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9999, 785)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxDxp-H88NPM",
        "colab_type": "text"
      },
      "source": [
        "## Image Details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RUOCz1-8MsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_width = 28\n",
        "image_height = 28\n",
        "image_size = image_width * image_height # 784\n",
        "labels = 10\n",
        "\n",
        "train_imgs = np.asfarray(train[:, 1:]) * (0.99 / 255) + 0.01\n",
        "test_imgs = np.asfarray(test[:, 1:]) * (0.99 / 255) + 0.01\n",
        "\n",
        "train_labels = np.asfarray(train[:, :1])\n",
        "test_labels = np.asfarray(test[:, :1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_Dbgi2h-6Iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = np.arange(10)\n",
        "\n",
        "# transform labels into one hot representation\n",
        "train_labels_one_hot = (lr==train_labels).astype(np.float)\n",
        "test_labels_one_hot = (lr==test_labels).astype(np.float)\n",
        "\n",
        "# we don't want zeroes and ones in the labels neither:\n",
        "train_labels_one_hot[train_labels_one_hot==0] = 0.01\n",
        "train_labels_one_hot[train_labels_one_hot==1] = 0.99\n",
        "test_labels_one_hot[test_labels_one_hot==0] = 0.01\n",
        "test_labels_one_hot[test_labels_one_hot==1] = 0.99"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSOl494P9JT2",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network Classifying the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzUNSBJI9tQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Source: https://www.python-course.eu/neural_network_mnist.php \n",
        "import numpy as np\n",
        "\n",
        "@np.vectorize\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.e ** -x)\n",
        "activation_function = sigmoid\n",
        "\n",
        "from scipy.stats import truncnorm\n",
        "\n",
        "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
        "    return truncnorm((low - mean) / sd, \n",
        "                     (upp - mean) / sd, \n",
        "                     loc=mean, \n",
        "                     scale=sd)\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "    \n",
        "    def __init__(self, \n",
        "                 no_of_in_nodes, \n",
        "                 no_of_out_nodes, \n",
        "                 no_of_hidden_nodes,\n",
        "                 learning_rate):\n",
        "        self.no_of_in_nodes = no_of_in_nodes\n",
        "        self.no_of_out_nodes = no_of_out_nodes\n",
        "        self.no_of_hidden_nodes = no_of_hidden_nodes\n",
        "        self.learning_rate = learning_rate \n",
        "        self.create_weight_matrices()\n",
        "        \n",
        "    def create_weight_matrices(self):\n",
        "        \"\"\" \n",
        "        A method to initialize the weight \n",
        "        matrices of the neural network\n",
        "        \"\"\"\n",
        "        rad = 1 / np.sqrt(self.no_of_in_nodes)\n",
        "        X = truncated_normal(mean=0, \n",
        "                             sd=1, \n",
        "                             low=-rad, \n",
        "                             upp=rad)\n",
        "        self.wih = X.rvs((self.no_of_hidden_nodes, \n",
        "                                       self.no_of_in_nodes))\n",
        "        rad = 1 / np.sqrt(self.no_of_hidden_nodes)\n",
        "        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
        "        self.who = X.rvs((self.no_of_out_nodes, \n",
        "                                         self.no_of_hidden_nodes))\n",
        "        \n",
        "    \n",
        "    def train(self, input_vector, target_vector):\n",
        "        \"\"\"\n",
        "        input_vector and target_vector can \n",
        "        be tuple, list or ndarray\n",
        "        \"\"\"\n",
        "        \n",
        "        input_vector = np.array(input_vector, ndmin=2).T\n",
        "        target_vector = np.array(target_vector, ndmin=2).T\n",
        "        \n",
        "        output_vector1 = np.dot(self.wih, \n",
        "                                input_vector)\n",
        "        output_hidden = activation_function(output_vector1)\n",
        "        \n",
        "        output_vector2 = np.dot(self.who, \n",
        "                                output_hidden)\n",
        "        output_network = activation_function(output_vector2)\n",
        "        \n",
        "        output_errors = target_vector - output_network\n",
        "        # update the weights:\n",
        "        tmp = output_errors * output_network \\\n",
        "              * (1.0 - output_network)     \n",
        "        tmp = self.learning_rate  * np.dot(tmp, \n",
        "                                           output_hidden.T)\n",
        "        self.who += tmp\n",
        "\n",
        "\n",
        "        # calculate hidden errors:\n",
        "        hidden_errors = np.dot(self.who.T, \n",
        "                               output_errors)\n",
        "        # update the weights:\n",
        "        tmp = hidden_errors * output_hidden * \\\n",
        "              (1.0 - output_hidden)\n",
        "        self.wih += self.learning_rate \\\n",
        "                          * np.dot(tmp, input_vector.T)\n",
        "        \n",
        "\n",
        "        \n",
        "    \n",
        "    def run(self, input_vector):\n",
        "        # input_vector can be tuple, list or ndarray\n",
        "        input_vector = np.array(input_vector, ndmin=2).T\n",
        "\n",
        "        output_vector = np.dot(self.wih, \n",
        "                               input_vector)\n",
        "        output_vector = activation_function(output_vector)\n",
        "        \n",
        "        output_vector = np.dot(self.who, \n",
        "                               output_vector)\n",
        "        output_vector = activation_function(output_vector)\n",
        "    \n",
        "        return output_vector\n",
        "            \n",
        "    def confusion_matrix(self, data_array, labels):\n",
        "        cm = np.zeros((10, 10), int)\n",
        "        for i in range(len(data_array)):\n",
        "            res = self.run(data_array[i])\n",
        "            res_max = res.argmax()\n",
        "            target = labels[i][0]\n",
        "            cm[res_max, int(target)] += 1\n",
        "        return cm    \n",
        "\n",
        "    def precision(self, label, confusion_matrix):\n",
        "        col = confusion_matrix[:, label]\n",
        "        return confusion_matrix[label, label] / col.sum()\n",
        "    \n",
        "    def recall(self, label, confusion_matrix):\n",
        "        row = confusion_matrix[label, :]\n",
        "        return confusion_matrix[label, label] / row.sum()\n",
        "        \n",
        "    \n",
        "    def evaluate(self, data, labels):\n",
        "        corrects, wrongs = 0, 0\n",
        "        for i in range(len(data)):\n",
        "            res = self.run(data[i])\n",
        "            res_max = res.argmax()\n",
        "            if res_max == labels[i]:\n",
        "                corrects += 1\n",
        "            else:\n",
        "                wrongs += 1\n",
        "        return corrects, wrongs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1I2CmLaDc6U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "4e7310ba-ba4a-4b78-9a76-6e2e6563896c"
      },
      "source": [
        "\n",
        "epochs = 10\n",
        "\n",
        "NN = NeuralNetwork(no_of_in_nodes = image_size, no_of_out_nodes = 10, no_of_hidden_nodes = 100, learning_rate = 0.1)\n",
        "\n",
        "for epoch in range(epochs):  \n",
        "    print(\"epoch: \", epoch)\n",
        "    for i in range(len(train_imgs)):\n",
        "        NN.train(train_imgs[i], \n",
        "                 train_labels_one_hot[i])\n",
        "  \n",
        "    corrects, wrongs = NN.evaluate(train_imgs, train_labels)\n",
        "    print(\"accuracy train: \", corrects / ( corrects + wrongs))\n",
        "    corrects, wrongs = NN.evaluate(test_imgs, test_labels)\n",
        "    print(\"accuracy: test\", corrects / ( corrects + wrongs))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  0\n",
            "accuracy train:  0.9495491591526526\n",
            "accuracy: test 0.9482948294829483\n",
            "epoch:  1\n",
            "accuracy train:  0.9659160986016434\n",
            "accuracy: test 0.9613961396139614\n",
            "epoch:  2\n",
            "accuracy train:  0.9699994999916666\n",
            "accuracy: test 0.961996199619962\n",
            "epoch:  3\n",
            "accuracy train:  0.9717995299921666\n",
            "accuracy: test 0.9638963896389638\n",
            "epoch:  4\n",
            "accuracy train:  0.9771662861047684\n",
            "accuracy: test 0.9658965896589659\n",
            "epoch:  5\n",
            "accuracy train:  0.9756829280488009\n",
            "accuracy: test 0.963996399639964\n",
            "epoch:  6\n",
            "accuracy train:  0.9778329638827313\n",
            "accuracy: test 0.9652965296529653\n",
            "epoch:  7\n",
            "accuracy train:  0.9781829697161619\n",
            "accuracy: test 0.9665966596659666\n",
            "epoch:  8\n",
            "accuracy train:  0.9778662977716295\n",
            "accuracy: test 0.9648964896489649\n",
            "epoch:  9\n",
            "accuracy train:  0.978632977216287\n",
            "accuracy: test 0.965996599659966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eRlk6FKD6uS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "12f3b133-0eee-4e6c-8f45-2ea59433027b"
      },
      "source": [
        "'''\n",
        "# Source : https://docs.google.com/presentation/d/1cYzq7TEGXRAhKF9P0eOI_q01kQAOajl-eS9h3CTGRLg/edit#slide=id.g5faeddb8e7_0_977\n",
        "np.random.seed(1)\n",
        "def relu(x):\n",
        "  return (x >= 0) * x\n",
        "\n",
        "def relu2deriv(output):\n",
        "  return output >= 0\n",
        "\n",
        "batch_size = 100\n",
        "allpha, iterations = (0.001, 300)\n",
        "pixels_per_image, num_labels, hidden_size = (784, 10, 100)\n",
        "\n",
        "#weights_0_1 = 0.2*np.random((pixels_per_image, hidden_size)) - 0.1\n",
        "#weights_1_2 = 0.2*np.random((hidden_size, num_labels)) - 0.1\n",
        "\n",
        "weights_0_1 = 0\n",
        "weights_1_2 = 0\n",
        "'''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Source : https://docs.google.com/presentation/d/1cYzq7TEGXRAhKF9P0eOI_q01kQAOajl-eS9h3CTGRLg/edit#slide=id.g5faeddb8e7_0_977\\nnp.random.seed(1)\\ndef relu(x):\\n  return (x >= 0) * x\\n\\ndef relu2deriv(output):\\n  return output >= 0\\n\\nbatch_size = 100\\nallpha, iterations = (0.001, 300)\\npixels_per_image, num_labels, hidden_size = (784, 10, 100)\\n\\n#weights_0_1 = 0.2*np.random((pixels_per_image, hidden_size)) - 0.1\\n#weights_1_2 = 0.2*np.random((hidden_size, num_labels)) - 0.1\\n\\nweights_0_1 = 0\\nweights_1_2 = 0\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6MDm8u5EjoT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "397b8b63-0bd0-4916-ee6a-1c10c4419ea7"
      },
      "source": [
        "'''\n",
        "# Source: https://docs.google.com/presentation/d/1cYzq7TEGXRAhKF9P0eOI_q01kQAOajl-eS9h3CTGRLg/edit#slide=id.g5faeddb8e7_0_977\n",
        "for j in range(iterations):\n",
        "  error, correct_cnt = (0.0, 0)\n",
        "  for i in range(int(len(train) / batch_size)):\n",
        "    batch_start, batch_end = ((i * batch_size), ((i+1) * batch_size))\n",
        "\n",
        "    layer_0 = train[batch_start: batch_end]\n",
        "    layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
        "    droupout_mask = np.random.randint(2, size=layer_1.shape)\n",
        "    layer_1 *= droupout_mask * 2\n",
        "    layer_2 = np.dot(layer_1, weights_1_2)\n",
        "\n",
        "    error += np.sum((labels[batch_start:batch_end] - layer_2) ** 2)\n",
        "    for k in range(batch_size):\n",
        "      correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start+k:batch_start+k+1]))\n",
        "      layer_2_delta = (labels[batch_start:batch_end] - layer_2) / batch_size\n",
        "      layer_1_delta = layer_2_delta.dot(weights_1_2.T)* relu2deriv(layer_1)\n",
        "      layer_1_delta *= droupout_mask\n",
        "\n",
        "      weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
        "      weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
        "\n",
        "  if(j%10 == 0):\n",
        "    test_error = 0.0\n",
        "    test_correct_cnt = 0\n",
        "\n",
        "    for i in range(len(test_image)):\n",
        "      layer_0 = test_images[i:i+1]\n",
        "      layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
        "      layer_2 = np.dot(layer_1, weights_1_2)\n",
        "'''"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Source: https://docs.google.com/presentation/d/1cYzq7TEGXRAhKF9P0eOI_q01kQAOajl-eS9h3CTGRLg/edit#slide=id.g5faeddb8e7_0_977\\nfor j in range(iterations):\\n  error, correct_cnt = (0.0, 0)\\n  for i in range(int(len(train) / batch_size)):\\n    batch_start, batch_end = ((i * batch_size), ((i+1) * batch_size))\\n\\n    layer_0 = train[batch_start: batch_end]\\n    layer_1 = relu(np.dot(layer_0, weights_0_1))\\n    droupout_mask = np.random.randint(2, size=layer_1.shape)\\n    layer_1 *= droupout_mask * 2\\n    layer_2 = np.dot(layer_1, weights_1_2)\\n\\n    error += np.sum((labels[batch_start:batch_end] - layer_2) ** 2)\\n    for k in range(batch_size):\\n      correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start+k:batch_start+k+1]))\\n      layer_2_delta = (labels[batch_start:batch_end] - layer_2) / batch_size\\n      layer_1_delta = layer_2_delta.dot(weights_1_2.T)* relu2deriv(layer_1)\\n      layer_1_delta *= droupout_mask\\n\\n      weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\\n      weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\\n\\n  if(j%10 == 0):\\n    test_error = 0.0\\n    test_correct_cnt = 0\\n\\n    for i in range(len(test_image)):\\n      layer_0 = test_images[i:i+1]\\n      layer_1 = relu(np.dot(layer_0, weights_0_1))\\n      layer_2 = np.dot(layer_1, weights_1_2)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ml9cxRueCr3",
        "colab_type": "text"
      },
      "source": [
        "## Hidden Layers (Incomplete)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G8wYyRHHeIN6",
        "colab": {}
      },
      "source": [
        "''' \n",
        "# Source: https://www.python-course.eu/neural_network_mnist.php \n",
        "import numpy as np\n",
        "\n",
        "@np.vectorize\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.e ** -x)\n",
        "activation_function = sigmoid\n",
        "\n",
        "from scipy.stats import truncnorm\n",
        "\n",
        "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
        "    return truncnorm((low - mean) / sd, \n",
        "                     (upp - mean) / sd, \n",
        "                     loc=mean, \n",
        "                     scale=sd)\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "    \n",
        "    def __init__(self, \n",
        "                 no_of_in_nodes, \n",
        "                 no_of_out_nodes, \n",
        "                 no_of_hidden_nodes,\n",
        "                 learning_rate):\n",
        "        self.no_of_in_nodes = no_of_in_nodes\n",
        "        self.no_of_out_nodes = no_of_out_nodes\n",
        "        self.no_of_hidden_nodes = no_of_hidden_nodes\n",
        "        self.learning_rate = learning_rate \n",
        "        self.create_weight_matrices()\n",
        "        \n",
        "    def create_weight_matrices(self):\n",
        "        \"\"\" \n",
        "        A method to initialize the weight \n",
        "        matrices of the neural network\n",
        "        \"\"\"\n",
        "        rad = 1 / np.sqrt(self.no_of_in_nodes)\n",
        "        X = truncated_normal(mean=0, \n",
        "                             sd=1, \n",
        "                             low=-rad, \n",
        "                             upp=rad)\n",
        "        self.wih = X.rvs((self.no_of_hidden_nodes, \n",
        "                                       self.no_of_in_nodes))\n",
        "        rad = 1 / np.sqrt(self.no_of_hidden_nodes)\n",
        "        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
        "        self.who = X.rvs((self.no_of_out_nodes, \n",
        "                                         self.no_of_hidden_nodes))\n",
        "        \n",
        "    \n",
        "    def train(self, input_vector, target_vector):\n",
        "        \"\"\"\n",
        "        input_vector and target_vector can \n",
        "        be tuple, list or ndarray\n",
        "        \"\"\"\n",
        "        \n",
        "        input_vector = np.array(input_vector, ndmin=2).T\n",
        "        target_vector = np.array(target_vector, ndmin=2).T\n",
        "        \n",
        "        output_vector1 = np.dot(self.wih, \n",
        "                                input_vector)\n",
        "        output_hidden = activation_function(output_vector1)\n",
        "        \n",
        "        output_vector2 = np.dot(self.who, \n",
        "                                output_hidden)\n",
        "        output_network = activation_function(output_vector2)\n",
        "        \n",
        "        output_errors = target_vector - output_network\n",
        "        # update the weights:\n",
        "        tmp = output_errors * output_network \\\n",
        "              * (1.0 - output_network)     \n",
        "        tmp = self.learning_rate  * np.dot(tmp, \n",
        "                                           output_hidden.T)\n",
        "        self.who += tmp\n",
        "\n",
        "\n",
        "        # calculate hidden errors:\n",
        "        hidden_errors = np.dot(self.who.T, \n",
        "                               output_errors)\n",
        "        # update the weights:\n",
        "        tmp = hidden_errors * output_hidden * \\\n",
        "              (1.0 - output_hidden)\n",
        "        self.wih += self.learning_rate \\\n",
        "                          * np.dot(tmp, input_vector.T)\n",
        "        \n",
        "\n",
        "        \n",
        "    \n",
        "    def run(self, input_vector):\n",
        "        # input_vector can be tuple, list or ndarray\n",
        "        input_vector = np.array(input_vector, ndmin=2).T\n",
        "\n",
        "        output_vector = np.dot(self.wih, \n",
        "                               input_vector)\n",
        "        output_vector = activation_function(output_vector)\n",
        "        \n",
        "        output_vector = np.dot(self.who, \n",
        "                               output_vector)\n",
        "        output_vector = activation_function(output_vector)\n",
        "    \n",
        "        return output_vector\n",
        "            \n",
        "    def confusion_matrix(self, data_array, labels):\n",
        "        cm = np.zeros((10, 10), int)\n",
        "        for i in range(len(data_array)):\n",
        "            res = self.run(data_array[i])\n",
        "            res_max = res.argmax()\n",
        "            target = labels[i][0]\n",
        "            cm[res_max, int(target)] += 1\n",
        "        return cm    \n",
        "\n",
        "    def precision(self, label, confusion_matrix):\n",
        "        col = confusion_matrix[:, label]\n",
        "        return confusion_matrix[label, label] / col.sum()\n",
        "    \n",
        "    def recall(self, label, confusion_matrix):\n",
        "        row = confusion_matrix[label, :]\n",
        "        return confusion_matrix[label, label] / row.sum()\n",
        "        \n",
        "    \n",
        "    def evaluate(self, data, labels):\n",
        "        corrects, wrongs = 0, 0\n",
        "        for i in range(len(data)):\n",
        "            res = self.run(data[i])\n",
        "            res_max = res.argmax()\n",
        "            if res_max == labels[i]:\n",
        "                corrects += 1\n",
        "            else:\n",
        "                wrongs += 1\n",
        "        return corrects, wrongs\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}